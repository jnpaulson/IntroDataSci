---
title: "Tidy Data"
author: "CMSC320"
date: "February 11, 2016"
output: html_document
---

```{r, echo=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

In this section we will principles of preparing and organizing data in a way that is amenable for analysis, both in modeling and visualization. We derive many of our ideas from the paper [Tidy Data](http://www.jstatsoft.org/v59/i10/paper) by Hadley Wickham. Associated with that paper we will use two very powerful R libraries `tidyr` and `dplyr` which are extremely useful in writing scripts for data cleaning, preparation and summarization. A basic design principle behind these libraries is trying to effectively and efficiently capture very common use cases and operations performed in data cleaning. The paper frames these use cases and operations which are them implemented in software.

Along the way we will also briefly touch on the relationship between this framework and classical database design, especially the Entity-Relationship model.

## Starting to think about data models

In the next few lectures we will start thinking in the abstract about how to represent structure of data and datasets. We think of a _data model_ as a collection of concepts that describes how data is represented and accessed. As we will see in the examples of packages `tidyr` and `dplyr`, thinking explicitly about the structure of datasets allows us to design and write general purpose and efficient code. Also, thinking abstractly of data structure, beyond a specific implemetation, makes it easier to share data across programs and systems, and integrate data from different sources.

Once we have thought about structure, we can then think about _semantics_: what does data represent? In this section, and in the course overall, we have thought about _structure_ and _semantics_ as follows:

- **Structure**: We have assumed that data is organized in rectangular data structures (tables with rows and columns)
- **Semantics**: We have discussed the notion of _values_, _attributes_, and _entities_. Recall that we can refer to _attributes_ as _variables_ and _entities_ as _observations_. In fact, the _Tidy Data_ paper uses the _observation_ and _variable_ terminology.

In our previous section on datatypes, and in this section we use the following _data semantics_: a dataset is a collection of _values_, numeric or categorical, organized into _entities_ (_observations_) and _attributes_ (_variables_). Each _attribute_ contains values of a specific measurement across _entities_, and _entities_ collect all measurements across _attributes_. 

## Tidy Data

With this structure and semantics on hand, we can define _tidy data_: this is a rectangular data structure where

1. Each attribute (or variable) forms a column  
2. Each entity (or observation) forms a row  
3. Each type of entity (observational unit) forms a table  


Here is an example of a tidy dataset: 

```{r}
library(nycflights13)
head(flights)
```

it has one observation per row, a single variable per column. Notice only information about flights are included here (e.g., no airport information other than the name) in these observations.

The rest of this section concerns common problems in data preparation, namely use cases commonly found in raw datasets that need to be addressed to turn messy data into tidy data. These would be operations that you would perform on data obtained as a csv file from a collaborator or data repository, or as the result of scraping data from webpages or other sources.


### Common problems in messy data

The set of common operations we will study are based on these common problems found in datasets. We will see each one in detail:

- Column headers are values, not variable names (gather)  
- Multiple variables stored in one column (split)  
- Variables stored in both rows and column (rotate)  
- Multiple types of observational units are stored in the same table (normalize)  
- Single observational unit stored in multiple tables (join)  


We are using data from Hadley's paper found in [github](https://github.com/hadley/tidyr). It's included in the course Rocker image in directory repository in directory `/home/rstudio/tidy_unit/example_data`:

```{r, eval=FALSE}
data_dir <- "/home/ids_materials/tidy_unit/example_data"
```

```{r, eval=TRUE, echo=FALSE}
data_dir <- "tidyr/vignettes"
```


Headers as values
------------------

The first problem we'll see is the case where a table header contains values. At this point we will introduce the `dplyr` package, which we'll use extensively in this course. It is an extremely powerful and efficient way of manipulating tidy data. It will serve as the core of our data manipulation knowledge after this course.

`dplyr` defines a slightly different way of using data.frames. The `tbl_df` function converts a standard R data.frame into a `tbl_df` defined by `dplyr`. One nice thing it does, for example, is print tables in a much friendlier way.

```{r}
library(tidyr)
library(dplyr)

pew <- tbl_df(read.csv(file.path(data_dir, "pew.csv"), stringsAsFactors=FALSE, check.names=FALSE))
pew
```

This table has the number of survey respondents of a specific religion that report their income within some range. A tidy version of this table would consider the *variables* of each observation to be `religion, income, frequency` where `frequency` has the number of respondents for each religion and income range. The function to use in the `tidyr` package is `gather`:

```{r}
tidy_pew <- gather(pew, income, frequency, -religion)
tidy_pew
```

This says: gather all the columns from the `pew` (except `religion`) into key-value columns `income` and `frequency`. This table is much easier to use in other analyses.

Another example: this table has a row for each song appearing in the billboard top 100. It contains track information, and the date it entered the top 100. It then shows the rank in each of the next 76 weeks.

```{r}
billboard <- tbl_df(read.csv(file.path(data_dir, "billboard.csv"), stringsAsFactors=FALSE))
billboard
```

Challenge:
This dataset has values as column names. Which column names are values? How do we tidy this dataset?

Multiple variables in one column
--------------------------------------

The next problem we'll see is the case when we see multiple variables in a single column. Consider the following dataset of tuberculosis cases:

```{r}
tb <- tbl_df(read.csv(file.path(data_dir, "tb.csv"), stringsAsFactors = FALSE))
tb
```

This table has a row for each year and strain of tuberculosis (given by the first two columns). The remaining columns state the number of cases for a given demographic. For example, `m1524` corresponds to males between 15 and 24 years old, and `f1524` are females age 15-24. As you can see each of these columns has two variables: `sex` and `age`.

Challenge: what else is untidy about this dataset?

So, we have to do two operations to tidy this table, first we need to use `gather` the tabulation columns into a `demo` and `n` columns (for demographic and number of cases):

```{r}
tidy_tb <- gather(tb, demo, n, -iso2, -year)
tidy_tb
```

Next, we need to `separate` the values in the `demo` column into two variables `sex` and `age`

```{r}
tidy_tb <- separate(tidy_tb, demo, c("sex", "age"), sep=1)
tidy_tb
```

This calls the `separate` function on table `tidy_db`, separating the `demo` variable into variables `sex` and `age` by separating each value after the first character (that's the `sep` argument).

So, if we put these two commands together we'll see a very common pattern in data manipulation:

```{r}
tidy_tb <- gather(tb, demo, n, -iso2, -year)
tidy_tb <- separate(tidy_tb, demo, c("sex", "age"), sep=1)
tidy_tb
```

We take a table, perform an operation, then take the result and perform another operation on it. This pattern is so common that `dplyr` provides a special syntax to make this more elegant:

```{r}
tidy_tb <- tb %>% 
  gather(demo, n, -iso2, -year)  %>%
  separate(demo, c("sex", "age"), sep=1)
tidy_tb
```

The special syntax `%>%` is called the `pipe` operator (it pipes the result of one operation into the next operation). Specifically, it takes whatever is on the left-hand side, and then puts it into the *first* argument of the function call in the right-hand side.

Challenge: Rewrite the following operations using the pipe operator:

```{r}
tidy_billboard <- gather(billboard, week, rank, wk1:wk76)
tidy_billboard <- separate(tidy_billboard, date.entered, 
                           c("year_entered", "month_entered", "day_entered"),
                           sep="-")
tidy_billboard
```

Variables stored in both rows and columns
-------------------------------------------

This is the messiest, commonly found type of data. Let's take a look at an example, this is daily weather data from for one weather station in Mexico in 2010.

```{r}
weather <- tbl_df(read.csv(file.path(data_dir, "weather.csv"), stringsAsFactors = FALSE))
weather
```

So, we have two rows for each month, one with maximum daily temperature, one with minimum daily temperature, the columns starting with `d` correspond to the day in the where the measurements were made.

Challenge: How would a tidy version of this data look like?

```{r}
weather %>%
  gather(day, value, d1:d31, na.rm=TRUE) %>%
  spread(element, value)
```

The new function we've used here is `spread`. It does the inverse of `gather` it spreads columns `element` and `value` into separate columns.

Multiple types in one table
----------------------------

Remember that an important aspect of tidy data is that it contains exactly one kind of observation in a single table. Let's see the billboard example again after the `gather` operation we did before:

```{r}
tidy_billboard <- billboard %>%
  gather(week, rank, wk1:wk76, na.rm=TRUE)
tidy_billboard
```

Let's sort this table by track to see a problem with this table:

```{r}
tidy_billboard <- tidy_billboard %>%
  arrange(track)
tidy_billboard
```

We have a lot of repeated information in many of these rows (the artist, track name, year, title and date entered). The problem is that this table contains information about both tracks and rank in billboard. That's two different kinds of observations that should belong in two different tables in a tidy dataset.

Let's make a song table that only includes information about songs:

```{r}
song <- tidy_billboard %>%
  select(artist, track, year, time, date.entered) %>%
  unique()
song
```

The `select` function takes a specific set of columns in a table. This is equivalent to the following syntax which we saw last time:

```{r}
tidy_billboard[,c("artist", "track", "year", "time", "date.entered")]
```

The purpose of `dplyr` is to make these type of expressions nicer to write and read. The `unique` function removes any duplicate rows in a table. That's how we have a single row for each song. 

Next, we would like to remove all the song information from the rank table. But we need to do it in a way that still remembers which song each ranking observation corresponds to. To do that, let's first give each song an identifier that we can use to link songs and rankings. So, we can produce the final version of our song table like this:

```{r}
song <- tidy_billboard %>%
  select(artist, track, year, time, date.entered) %>% 
  unique() %>%
  mutate(song_id = row_number())
song
```

The `mutate` function adds a new column to the table, in this case with column name `song_id` and value the row number the song appears in the table (from the `row_number` column).

Now we can make a rank table, we combine the tidy billboard table with our new song table using a `join` (we'll learn all about joins later today). It checks the values on each row of the billboard table and looks for rows in the song table that have the exact same values, and makes a new row that combines the information from both tables. 

```{r}
tidy_billboard %>%
  left_join(song, c("artist", "year", "track", "time", "date.entered"))
```

That adds the `song_id` variable to the `tidy_billboard` table. So now we can remove the song information and only keep ranking information and the `song_id`.

```{r}
rank <- tidy_billboard %>%
  left_join(song, c("artist", "year", "track", "time", "date.entered")) %>%
  select(song_id, week, rank)
rank
```

Challenge:
Let's do a little better job at tidying the billboard dataset:

1. When using `gather` to make the `week` and `rank` columns, remove any weeks where the song does not appear in the top 100. This is coded as missing (`NA`). See the `na.rm` argument to `gather`.  
2. Make `week` a numeric variable (i.e., remove `wk`). See what the `extract_numeric` function does.  
3. Instead of `date.entered` add a `date` column that states the actual date of each ranking. See how R deals with dates `?Date` and how you can turn a string into a `Date` using `as.Date`.  
4. Sort the resulting table by date and rank.
5. Make new `song` and `rank` tables. `song` will now not have the `date.entered` column, and `rank` will have the new `date` column you have just created.

## Tidy data, the ER model and normal form

There is a statement above, drawn from the Tidy Data paper, that makes the parallel between the definition of tidy data used here and _Codd's third normal form_. The Entity-Relationship data model is a fundamental way of describing data and datasets. In the database community, it is a cornerstone of how databases are designed. It has origins in set theory which allows an elegant formal way of describing data in a _data independent_ way. This elegant formalism allows users don't have to think about how data is physically stored, or processed. 

The fundamental objects in this formalism are _entities_ and their _attributes_, as we have seen before, and _relationships_ and _relationship attributes_ which we saw briefly in the example above, where 'rankings' and 'songs' are distinct types of entities and we define _relationships_ between them.

Frequently, entities, their attributes, and relationships between their attributes are described in ER diagrams like this one:

![](er.png)

Here, rectangles are _entitites_, diamonds and edges indicate _relationships_. Circles describe either entity or relationship _attributes_. The notion of _normal form_ in this formalism allows users to reason about and remove data redundancies. 

In summary, the notion of _tidy data_ has a nice parallel in a very elegant formalism commonly used in the database filed. However, this formalism can extend beyond what we need for data analysis. Many features of this formalism are more applicable to data management issues, especially consistency and redundancy. 