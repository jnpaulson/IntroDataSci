---
title: 'Introduction to Statistical Learning: Preliminaries'
author: "CMSC320"
date: "March 7, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The purpose of this class is for you to learn Statistical and Machine Learning techniques commonly used in data analysis. By the end of the term, you should be able to read papers that used these methods critically and analyze data using them.

When using any of these tools we will be we will be asking ourselves if our findings are "statistically significant". For example, if we make use of a classification algorithm and find that we can correctly predict an outcome in 70 out of our 100 cases, how can we determine if this could have happened by chance alone? To be able to answer these questions, we need to understand some basic probabilistic and statistical principles. In this section we will review some of these principles.

## Variation, randomness and stochasticity

In the preceeding sections of the class we have not spoken too much about randomness and stochasticity. We have spoken about _variation_ though. When we discussed the notion of _spread_ in a given dataset, measured by the sample standard deviation, for example, we are referring to the fact that in a population of entities (e.g., images of cats) there is naturally occuring variation in measurements. Notice that we can discuss the notation of _variation_ without referring to any randomness, stochasticity or noise. 

Why probability then? Because, we do want to distinguish, when possible, between natural occuring variation and randomness or stochasticity. For instance, suppose we want to learn something about education loan debt for 19-30 year olds in Maryland. We could find loan debt for **all** 19-30 year old Maryland residents, and calculate average and standard deviation. But that's difficult to do for all residents. So, instead we sample (say by randomly sending Twitter surveys), and _estimate_ the average and standard deviation of debt in this population from the sample. The issue is, we could do the same from a different random sample and get a different set of estimates. Why? Because there is naturally-occuring variation in this population.  

So, a simple question to ask is, how good are our _estimates_ of debt mean and standard deviation from sample of 19-30 year old Marylanders? 

Now, suppose we build a predictive model of loan debt for 19-30 year old Marylanders based on other variables (e.g., sex, income, education, wages, etc.) from our sample. How good will this model perform when predicting debt in general?

We use probability and statistics to answer these questions. We use probability to capture stochasticity in the sampling process and model naturally occuring variation in measurements in a population of interest.

One final word, the term _population_ which we use extensively here means **the entire** collection of entities we want to model. This could include people, but also images, text, GO positions, etc.

One may be interested in various things: What effects do the covariates have on the outcome? How well can we describe these effects? Can we predict the outcome using the covariates?, etc...

Linear Regression
------------------

Linear regression is the most common approach for describing the relation between predictors (or covariates) and outcome.  Here we will see how regression relates to prediction.

Let's start with a simple example. Let's say we have a random sample of US males and we record their heights ($Y$) and the average height of their parents ($X$).

Say we pick a random subject. How would you predict their height? 

What if I told you the average height of their parents?  Would your strategy for predicting change?

```{r, echo=FALSE}
library(MASS)
set.seed(1)
x <- mvrnorm(2000, mu=c(70,69), Sigma=matrix(c(3^2, .5*3*3, .5*3*3, 3^2), 2, 2))
```

```{r, echo=FALSE}
par(mgp=c(1.25,.5,0), mar=c(2.25, 2.1, 1, 1))
plot(x, xlab="Parent Height", ylab="Height", cex=.5, col="darkblue")
abline(v=mean(x[,1]), h=mean(x[,2]), lty=2, col="red")
```

We can show mathematically that for a particular definition of "best", the average is the best predictor of a value picked from that population. However, if we have information about a related variable, then the conditional average is best.

One can think of the conditional average as the average heights for all men with parents of particular average height.

```{r, echo=FALSE}
X0 <- 73
par(mgp=c(1.25,.5,0), mar=c(2.25, 2.1, 1, 1))
plot(x, xlab="Parent Height", ylab="Height", cex=.5, col="darkblue")
abline(v=mean(x[,1]), h=mean(x[,2]), lty=2, col="red")
abline(v=c(X0-.5,X0+.5), col="red", lwd=2)
X <- x[,1]
Y <- x[,2]
fit1 <- lm(Y~X)
points(X0, predict(fit1, newdata=list(X=X0)), col="orange", pch="A", cex=2)
```

In the case of height and parent height, the data actually look bivariate normal (football shaped) and one can show that the best predictor (the conditional average) of height given parent height is

![Galton](galton.jpg)

$$
E[Y|X=x] = \mu_Y + \rho \frac{\sigma_Y}{\sigma_X} (x - \mu_X)
$$

with $\mu_X=E[X]$ (average parent height), $\mu_Y=E[Y]$ (average height), and where $\rho$ is the correlation coefficient of height and weight. Notice that this is a **linear function** of height!

If we obtain a random sample of the data, then each of the above parameters is substituted by the sample estimates and we get a familiar expression:

$$
\hat{Y}(x) = \bar{Y} + r \frac{SD_Y}{SD_X}(x-\bar{X}).
$$

```{r, echo=FALSE}
par(mgp=c(1.25,.5,0), mar=c(2.25, 2.1, 1, 1))
plot(x, xlab="Parent Height", ylab="Height", cex=.5, col="darkblue")
abline(v=mean(x[,1]), h=mean(x[,2]), lty=2, col="red")
abline(v=c(X0-.5,X0+.5), col="red", lwd=2)
abline(fit1, col="orange", lwd=2)
points(X0, predict(fit1, newdata=list(X=X0)), col="orange", pch="A", cex=2)
```

Linear regression is popular mainly because of the interpretability of the parameters. It allows us to perform _inference_ about our measurements. E.g.,

- Which predictors are associated with the response?
- What is the relationship between the response and each predictor?
- Can the relationship between $Y$ and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?

However, the interpretation only makes sense if the model is an appropriate approximation of the natural data generating process. It is likely that the linear regression model from a randomly selected publication will do a terrible job at predicting results in data where the model was not trained on. Prediction is not really given much importance in many scientific fields, e.g. Epidemiology and Social Sciences. In other fields, e.g. Surveillance, Finance and web-commerce it is everything. Notice that in the fields where prediction is important, linear regression is not as popular.

Prediction
-----------

Methods for prediction can be divided into two general groups: continuous and discrete outcomes:

1. When the outcome is discrete we will refer to it as *classification*. 
2. When the outcome is continuous we will refer to it as *regression*. 

These seem very different but they have some in common. In this class, we will talk about the commonalities, but in general, we will discuss these two cases separately.

The main common characteristic in both cases is that we observe predictors $X_1,\ldots,X_p$ and we want to predict the outcome (or response) $Y$.

Note: I will use $X$ to denote the vector of all predictors. So, $X_i$ are the predictors for the $i$-th subject and can include age, gender, ethnicity, etc.

Note: Given a prediction method we will use  $f(x)$ to the denote the prediction we get if the predictors are $X=x$.

Q: What are examples of prediction problems?

So, what does it mean to predict well? Let's look at the continuous data case first.

If I have a prediction $f(X)$ based on predictors $X$, how do I define a "good prediction" mathematically. A common way of defining closeness is with squared error:

$$
L\{Y,f(X)\} = \{Y-f(X)\}^2.
$$

We sometime call this the *loss function*.

Many times, it is hard to believe that the linear regresion model holds. A simple example comes from AIDS research:

```{r, echo=FALSE}
CD4_data <- read.table("cd4.data", col.names=c("Time", "CD4", "Age", "Packs", "Drugs", "Sex", "Cesd", "ID"))
par(mfrow=c(1,1))
with(CD4_data, 
     plot(Time, CD4, pch=".", main="CD4 counts vs. Time", xlab="Time since seroconversion", ylab="CD4"))
```

Other settings
-----------

A major focus of this class is prediction, or *supervised learning*. However, we will also see a few other learning settings. For instance, suppose we only observe vectors of random variables, $X_1,\ldots,X_p$ but no outcome $Y$? In this case we still want to find some informative structure (e.g. *clustering*). This setting is called *unsupervised learning*. We can include probability density estimation under this setting.

Terminology and notation
---------------------

We will be mixing the terminology of statistics and computer
science. For example, we will sometimes call $Y$ and $X$ the
outcome/predictors, sometimes observed/covariates, and even
input/output.

We will sometimes denote predictors with $X$ and outcomes/responses with $Y$
(quantitative) and $G$ (qualitative). Notice $G$ are not numbers, so
we cannot add or multiply them.

Height and weight are *quantitative measurements*. These are sometimes
called continuous measurements.

Gender is a *qualitative measurement*. They are also called
categorical or discrete. This is a particularly simple example because
there are only two values. With two values we sometimes call it
*binary*. We will use $G$ to denote the set of possible
values. For gender it would be $G=\{Male,Female\}$. A special
case of qualitative variables are *ordered qualitative* where one can
impose an order. With men/women this can't be done, but with, say,
$G=\{low,medium,high\}$ it can.

A regression problem
-----------------

Recall the example data from AIDS research mentioned previously. Here
we are plotting the data along with a curve from which data could have
plausibly been generated. 

```{r, echo=FALSE}
fit <- loess(CD4~Time, data=CD4_data)
f <- function(x) predict(fit, newdata=data.frame(Time=x))

x <- seq(-2, 4, len=500)
with(CD4_data,
     plot(Time, CD4, xlim=c(-2,4), ylim=c(0,3000), main="CD4 counts vs. Time", xlab="Time since seroconversion", ylab="CD4", cex=.6))
lines(x, f(x), col="blue", lwd=2)
```

For now, let's consider this curve as truth and simulate CD4 counts from it.
We will use this simulated data to compare two simple but commonly
used methods to predict $Y$ (CD4 counts) from $X$ (Time), and discuss some of the issues that
will arise throughout this course. In particular, what is overfitting.

```{r, echo=FALSE}
make.x <- function(n)
  {
    runif(n, min=-2, max=4)
  }

train.xx <- make.x(500)
test.xx <- make.x(100)

train.y <- 3*f(train.xx) + rnorm(length(train.xx),sd=250)
test.y <- 3*f(test.xx) + rnorm(length(test.xx), sd=250)

train.df <- data.frame(x=train.xx, y=train.y)
test.df <- data.frame(x=test.xx, y=test.y)
```

```{r, echo=FALSE}
plot(train.xx, train.y, cex=.6, main="CD4 counts vs. Time", xlab="Time since zero conversion", ylab="CD4")
lines(x,3*f(x), col="blue", lwd=2)
```

### Linear regression

Probably the most used method in data analysis. In this case, we predict
the output $Y$ via the model

$$
Y = \beta_0 + \beta_1 X.
$$
However, we do not know what $\beta_0$ or $\beta_1$ are.

We use the a sample of training data to *estimate* them. We can also say we train
the model on the data to get numeric coefficients. We will use the hat
to denote the estimates: $\hat{\beta_0}$ and $\hat{\beta_1}$.

We will start using $\mathbf{\beta}$ to denote the vector
$(\beta_0,\beta_1)'$. We would call these the _parameters_
of the model.

The most common way to estimates $\beta$s is by least squares. In
this case, we choose the $\mathbf{\beta}$ that minimizes

$$
RSS(\mathbf{\beta}) = \sum_{i=1}^N \{y_i - (\beta_0 + \beta_1 X_i)\}^2.
$$

Remember our discussion of the sample mean? The value of $\beta$ can be found using mathematically. Notice we can predict $Y$ for any $X$:

$$
\hat{Y} = \hat{\beta_0} + \hat{\beta_1} X
$$

The next Figure shows the prediction graphically. However, the data seems to suggest we could do
better by considering more flexible models.

```{r, echo=FALSE}
lmfit <- lm(y~x, data=train.df)
plot(train.xx, train.y, cex=.6, main="CD4 counts vs. Time", xlab="Time since seroconversion", ylab="CD4")
abline(lmfit, col="red", lwd=2)
```

### K-nearest neighbor

Nearest neighbor methods use the points closest in predictor space to
$x$ to obtain an estimate of $Y$. For the K-nearest neighbor method
(KNN) we define

$$
\hat{Y} = \frac{1}{k} \sum_{x_k \in N_k(x)} y_k.
$$

Here $N_k(x)$ contains the $k$-nearest points to $x$. Notice, as for
linear regression, we can predict $Y$ for any $X$.

In the next Figure we see the results of KNN using the 15 nearest
neighbors. This estimate looks better than the linear model.

```{r, echo=FALSE}
source("knn.R")
knny15 <- knnReg(train.df$x, x, train.df$y, k=15)
plot(train.xx, train.y, cex=.6, main="CD4 counts vs. Time", xlab="Time since seroconversion", ylab="CD4")
abline(lmfit, col="red", lwd=2)
lines(x, knny15, col="orange", lwd=2)
```

We do better with KNN than with linear regression. However, we have to be
careful about *overfitting*.

Roughly speaking, *overfitting* is when you mold an algorithm to work
very well (sometimes perfect) on a particular data set forgetting that
it is the outcome of a random process and our trained algorithm may
not do as well in other instances.

Next, we see what happens when we use KNN with k=1. In this case we
make no mistakes in prediction, but
do we really believe we can do well in general with this estimate?

```{r, echo=FALSE}
knny1 <- knnReg(train.df$x, x, train.df$y, k=1)
plot(train.xx, train.y, cex=.6, main="CD4 counts vs. Time", xlab="Time since seroconversion", ylab="CD4")
abline(lmfit, col="red", lwd=2)
lines(x, knny1, col="purple", lwd=2, lty=2)
```

It turns out we have been hiding a *test* data set. Now we can see
which of these trained algorithms performs best on an independent test
set generated by the same stochastic process. 

```{r, echo=FALSE}
plot(test.xx, test.y, main="CD4 counts vs. Time", xlab="Time since seroconversion", ylab="CD4")
abline(lmfit, col="red", lwd=2)
lines(x, knny15, col="orange", lwd=2)
lines(x, knny1, col="purple", lwd=2 ,lty=2)
```

```{r, echo=FALSE}
rss <- function(f,y) mean((f-y)^2)

lm.train.rss <- rss(predict(lmfit), train.df$y)
knn15.train.rss <- rss(knnReg(train.df$x, train.df$x, train.df$y, k=15), train.df$y)
knn1.train.rss <- rss(knnReg(train.df$x, train.df$x, train.df$y, k=1), train.df$y)

lm.test.rss <- rss(predict(lmfit, newdata=test.df), test.df$y)
knn15.test.rss <- rss(knnReg(train.df$x, test.df$x, train.df$y, k=15), test.df$y)
knn1.test.rss <- rss(knnReg(train.df$x, test.df$x, train.df$y, k=1), test.df$y)

res <- data.frame(Method=c("Linear", "K=15", "K=1"),
  `Train set`=c(lm.train.rss, knn15.train.rss, knn1.train.rss)/1e3,
  `Test set`=c(lm.test.rss, knn15.test.rss, knn1.test.rss)/1e3)
```

We can see how good our predictions are using RSS again.

```{r, results="asis", echo=FALSE, cache=FALSE}
knitr::kable(res, digits=2)
```

Notice RSS is worse in test set than in the training set for the KNN methods. Especially for KNN=1. The
spikes we had in the estimate to predict the training data perfectly
no longer helps.

Smaller $k$ give more flexible estimates, but too much flexibility can
result in over-fitting and thus estimates with more variance. Larger
$k$ will give more stable estimates but may not be flexible
enough. Not being flexible is related to being biased. 

The next figure shows the RSS in the test and training sets for KNN
with varying k. Notice that for small $k$ we are clearly overfitting.

```{r, echo=FALSE, results='hide'}
ks <- ceiling(seq(1,50,len=10))
knnrss <- matrix(0.0, nr=length(ks), nc=2)
for (i in seq_along(ks)) {
  cat(ks[i])
  knnrss[i,1] <- rss(knnReg(train.df$x, train.df$x, train.df$y, k=ks[i]), train.df$y)
  knnrss[i,2] <- rss(knnReg(train.df$x, test.df$x, train.df$y, k=ks[i]), test.df$y)
}

knnrss <- knnrss/1e3
```

```{r, echo=FALSE}
plot(ks,knnrss[,1], type="b", col="blue", ylim=range(knnrss), xlab="k", ylab="rss")
points(ks, knnrss[,2], type="b", col="red")
legend("topright", c("train rss", "test rss"), lty=1, col=c("blue","red"))
```

### An illustration of the bias-variance tradeoff

A general concern in applying these types of models is how we trade off _bias_ and _variance_. We will see that statistical models can be of more or less flexibility. Linear regression has little flexibility, so we say that it is a model with high _bias_. On the other hand, $k$-nearest neighbors is a model with high flexibility, or small _bias_. However there is a tradeoff because models with low bias tend to change the most when applied to different data samples, we say they have high _variance_. Alternatively, models with high _bias_ tend to have low _variance_.

The next figure illustrates the bias/variance tradeoff. Here we make
boxplots of $f(1)-\hat{f}(1)$, where $\hat{f}(1)$ is the estimate
for each method trained on 1000 simulations. 

```{r, echo=FALSE}
nsim <- 500
ntrain <- 100

x <- replicate(nsim, make.x(ntrain))
true.f <- apply(x, 2, f)
y <- true.f + matrix(rnorm(nsim*ntrain, sd=250), nc=nsim)

simres <- matrix(0.0, nr=nsim, nc=3)

for (j in 1:nsim) {
  df <- data.frame(x=x[,j], y=y[,j])
  lmfit <- lm(y~x, data=df)
  simres[j,1] <- predict(lmfit, newdata=data.frame(x=1))
  simres[j,2] <- knnReg(x[,j], 1, y[,j], k=15)
  simres[j,3] <- knnReg(x[,j], 1, y[,j], k=1)
}

colnames(simres) <- c("linear", "k=15", "k=1")
```

```{r, echo=FALSE}
boxplot(simres-f(1), ylab="prediction - truth")
```

We can see that the prediction from the linear model is consistently
inaccurate. That is, it is _biased_, but stable (little variance). For $k=1$
we get the opposite, there is a lot of variability, but once in a
while it is very accurate (unbiased). For $k=15$ we get a decent
tradeoff of the two.

In this case we have simulated data as follows: for a
given $x$

$$
Y = f(x) + \epsilon
$$
where $f(x)$ is the "true" curve we are using and $\epsilon$ is normal
with mean zero and some variance $\sigma^2$.

We have been measuring how good our predictions are by using
RSS. Recall that we sometimes refer to this as a
*loss function*. In our case, we can't do better than $\sigma^2$
when minimizing this loss function. That is, we can't do better than this, on average,
with any other predictor. This is called _irreducible error_.

*Predictions are not always perfect.*


